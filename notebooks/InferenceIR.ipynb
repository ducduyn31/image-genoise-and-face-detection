{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDifKZNKV-mD",
        "outputId": "a4936418-f21c-418d-d525-422c85c61c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DiffBIR' already exists and is not an empty directory.\n",
            "--2023-11-06 16:02:19--  https://huggingface.co/lxq007/DiffBIR/resolve/main/face_swinir_v1.ckpt\n",
            "Resolving huggingface.co (huggingface.co)... 18.239.50.80, 18.239.50.103, 18.239.50.49, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.239.50.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/89/b5/89b59be81dd9ee5dad9cba60a24922c3ebe665e87e980fa7d92ef70fd30ae63c/62fb6605702e6ad35712299a23cc19ea7cd2fb4bbd50a8080fcefa8ad2620c71?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27face_swinir_v1.ckpt%3B+filename%3D%22face_swinir_v1.ckpt%22%3B&Expires=1699545739&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5OTU0NTczOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84OS9iNS84OWI1OWJlODFkZDllZTVkYWQ5Y2JhNjBhMjQ5MjJjM2ViZTY2NWU4N2U5ODBmYTdkOTJlZjcwZmQzMGFlNjNjLzYyZmI2NjA1NzAyZTZhZDM1NzEyMjk5YTIzY2MxOWVhN2NkMmZiNGJiZDUwYTgwODBmY2VmYThhZDI2MjBjNzE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=QQsRCEgvSnkjUZgRBZL52ovep4s4pm5uAcq1oWojtSGnz7qKoqhlbLpFDJvpOCygT9xSVo10nqPL0goz4zTM0P3FAxwKnwim6FHCzkiVgDOZc3cI6DBIIWQio9m%7E%7EYHeobJqTQF6xvawJ4agU5pNZ2X4imolFdeH8aWK7SqdLXvhVZBAT25gmYUwUpztJivMzRRzpNhYpn37gk0qEr39Rw45RPFfk67I77xGlsaSY5Mbp1bLG-qNHbD32NJaZKtKpuklv4G8DrAe0G5zWIaRAY8c28T3xeFCIqFc3jbt7WIXPiPQdbO8yupSvfVwqKWeLcyyF70dRr9mh4vGLez2Lw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-11-06 16:02:19--  https://cdn-lfs.huggingface.co/repos/89/b5/89b59be81dd9ee5dad9cba60a24922c3ebe665e87e980fa7d92ef70fd30ae63c/62fb6605702e6ad35712299a23cc19ea7cd2fb4bbd50a8080fcefa8ad2620c71?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27face_swinir_v1.ckpt%3B+filename%3D%22face_swinir_v1.ckpt%22%3B&Expires=1699545739&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5OTU0NTczOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84OS9iNS84OWI1OWJlODFkZDllZTVkYWQ5Y2JhNjBhMjQ5MjJjM2ViZTY2NWU4N2U5ODBmYTdkOTJlZjcwZmQzMGFlNjNjLzYyZmI2NjA1NzAyZTZhZDM1NzEyMjk5YTIzY2MxOWVhN2NkMmZiNGJiZDUwYTgwODBmY2VmYThhZDI2MjBjNzE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=QQsRCEgvSnkjUZgRBZL52ovep4s4pm5uAcq1oWojtSGnz7qKoqhlbLpFDJvpOCygT9xSVo10nqPL0goz4zTM0P3FAxwKnwim6FHCzkiVgDOZc3cI6DBIIWQio9m%7E%7EYHeobJqTQF6xvawJ4agU5pNZ2X4imolFdeH8aWK7SqdLXvhVZBAT25gmYUwUpztJivMzRRzpNhYpn37gk0qEr39Rw45RPFfk67I77xGlsaSY5Mbp1bLG-qNHbD32NJaZKtKpuklv4G8DrAe0G5zWIaRAY8c28T3xeFCIqFc3jbt7WIXPiPQdbO8yupSvfVwqKWeLcyyF70dRr9mh4vGLez2Lw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.65.82.11, 18.65.82.58, 18.65.82.63, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.65.82.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 90204233 (86M) [binary/octet-stream]\n",
            "Saving to: ‘DiffBIR/weights/face_swinir_v1.ckpt.1’\n",
            "\n",
            "face_swinir_v1.ckpt 100%[===================>]  86.03M   105MB/s    in 0.8s    \n",
            "\n",
            "2023-11-06 16:02:20 (105 MB/s) - ‘DiffBIR/weights/face_swinir_v1.ckpt.1’ saved [90204233/90204233]\n",
            "\n",
            "--2023-11-06 16:02:20--  https://huggingface.co/lxq007/DiffBIR/resolve/main/general_swinir_v1.ckpt\n",
            "Resolving huggingface.co (huggingface.co)... 18.239.50.80, 18.239.50.103, 18.239.50.49, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.239.50.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/89/b5/89b59be81dd9ee5dad9cba60a24922c3ebe665e87e980fa7d92ef70fd30ae63c/577b8f84815d324e5ee17ef3c85d8bc9b7ebd05b20a6d61a5452b66e7e24dcaf?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27general_swinir_v1.ckpt%3B+filename%3D%22general_swinir_v1.ckpt%22%3B&Expires=1699545741&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5OTU0NTc0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84OS9iNS84OWI1OWJlODFkZDllZTVkYWQ5Y2JhNjBhMjQ5MjJjM2ViZTY2NWU4N2U5ODBmYTdkOTJlZjcwZmQzMGFlNjNjLzU3N2I4Zjg0ODE1ZDMyNGU1ZWUxN2VmM2M4NWQ4YmM5YjdlYmQwNWIyMGE2ZDYxYTU0NTJiNjZlN2UyNGRjYWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=AEogfNKhsP-ovkcYwhajXiHPUhMly1n7kY2qUTX7ej2WsQYH-4sesXFekmxtt1WiGQm%7EfMFFH3nI0o1tOO78SzUNkAt6b%7EyG68d-bFrYbrJc3yerWq547d9iuDH%7Eah9suI53xRXfSpgSGcrvLPTTVKh81s-tEEacQdO3rH6zAmVqywLI7qjUkyE1vfC4ZZ1beiUhGkrv9lMwq4G4cyBkvlIKgsqUgs-uzVdyEasjtxYFJQACDBTPRB11cdeCFtVhIoxjtvbmA6B%7E1qQkk2Ps2AXvuxhelNf4szTIix2BTDHT3ECJOXWO4jNygi3MNkeiMP9JmXbFovCrJGxUYCaEmA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-11-06 16:02:21--  https://cdn-lfs.huggingface.co/repos/89/b5/89b59be81dd9ee5dad9cba60a24922c3ebe665e87e980fa7d92ef70fd30ae63c/577b8f84815d324e5ee17ef3c85d8bc9b7ebd05b20a6d61a5452b66e7e24dcaf?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27general_swinir_v1.ckpt%3B+filename%3D%22general_swinir_v1.ckpt%22%3B&Expires=1699545741&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5OTU0NTc0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84OS9iNS84OWI1OWJlODFkZDllZTVkYWQ5Y2JhNjBhMjQ5MjJjM2ViZTY2NWU4N2U5ODBmYTdkOTJlZjcwZmQzMGFlNjNjLzU3N2I4Zjg0ODE1ZDMyNGU1ZWUxN2VmM2M4NWQ4YmM5YjdlYmQwNWIyMGE2ZDYxYTU0NTJiNjZlN2UyNGRjYWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=AEogfNKhsP-ovkcYwhajXiHPUhMly1n7kY2qUTX7ej2WsQYH-4sesXFekmxtt1WiGQm%7EfMFFH3nI0o1tOO78SzUNkAt6b%7EyG68d-bFrYbrJc3yerWq547d9iuDH%7Eah9suI53xRXfSpgSGcrvLPTTVKh81s-tEEacQdO3rH6zAmVqywLI7qjUkyE1vfC4ZZ1beiUhGkrv9lMwq4G4cyBkvlIKgsqUgs-uzVdyEasjtxYFJQACDBTPRB11cdeCFtVhIoxjtvbmA6B%7E1qQkk2Ps2AXvuxhelNf4szTIix2BTDHT3ECJOXWO4jNygi3MNkeiMP9JmXbFovCrJGxUYCaEmA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.239.18.94, 18.239.18.29, 18.239.18.84, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.239.18.94|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 90208287 (86M) [binary/octet-stream]\n",
            "Saving to: ‘DiffBIR/weights/general_swinir_v1.ckpt.1’\n",
            "\n",
            "general_swinir_v1.c 100%[===================>]  86.03M   238MB/s    in 0.4s    \n",
            "\n",
            "2023-11-06 16:02:21 (238 MB/s) - ‘DiffBIR/weights/general_swinir_v1.ckpt.1’ saved [90208287/90208287]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/XPixelGroup/DiffBIR.git\n",
        "!mkdir -p DiffBIR/weights\n",
        "!wget https://huggingface.co/lxq007/DiffBIR/resolve/main/face_swinir_v1.ckpt -P DiffBIR/weights\n",
        "!wget https://huggingface.co/lxq007/DiffBIR/resolve/main/general_swinir_v1.ckpt -P DiffBIR/weights\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q drive/MyDrive/colab_mount/aug_widerface.zip"
      ],
      "metadata": {
        "id": "RAmuNKITWKVQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchvision torch timm tqdm pytorch_lightning einops lpips omegaconf"
      ],
      "metadata": {
        "id": "wc8XaMULY8Ac"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "yiQE0E0paK_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "from os.path import abspath, expanduser\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional, Callable, Dict, Union, List, Any, Set, overload\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from torch import nn, optim\n",
        "from torch.nn.functional import mse_loss\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "from PIL import Image\n",
        "from torchvision import tv_tensors, utils\n",
        "from torchvision.datasets import VisionDataset, wrap_dataset_for_transforms_v2\n",
        "from torchvision.datasets.utils import verify_str_arg\n",
        "from torchvision.transforms import v2\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "from torchvision.tv_tensors._dataset_wrapper import WRAPPER_FACTORIES\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from einops import rearrange\n",
        "from lpips import LPIPS"
      ],
      "metadata": {
        "id": "LXy5iiyRaMgM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "uQX75gCfY3qB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AugmentedDataset(VisionDataset):\n",
        "    BASE_FOLDER = \"aug_widerface\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 root: str,\n",
        "                 split: str = \"train\",\n",
        "                 transform: Optional[Callable] = None,\n",
        "                 target_transform: Optional[Callable] = None,\n",
        "                 ) -> None:\n",
        "        super().__init__(\n",
        "            root=os.path.join(root, self.BASE_FOLDER), transform=transform, target_transform=target_transform\n",
        "        )\n",
        "\n",
        "        self.split = verify_str_arg(split, \"split\", (\"train\", \"val\", \"test\"))\n",
        "\n",
        "        if not self._check_integrity():\n",
        "            raise RuntimeError(\"Dataset not found or corrupted.\")\n",
        "\n",
        "        self.img_info: List[Dict[str, Union[str, Dict[str, torch.Tensor]]]] = []\n",
        "        if self.split in (\"train\", \"val\"):\n",
        "            self.parse_train_val_annotations_file()\n",
        "        else:\n",
        "            self.parse_test_annotations_file()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.img_info[index][\"img_path\"])\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        target = None if self.split == \"test\" else self.img_info[index][\"annotations\"]\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_info)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        lines = [\"Split: {split}\"]\n",
        "        return \"\\n\".join(lines).format(**self.__dict__)\n",
        "\n",
        "    def _check_integrity(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    def parse_train_val_annotations_file(self) -> None:\n",
        "        filename = \"aug_wider_face_train_bbx_gt.txt\" if self.split == \"train\" else \"aug_wider_face_val_bbx_gt.txt\"\n",
        "        filepath = os.path.join(self.root, \"AUG_wider_face_split\", filename)\n",
        "\n",
        "        with open(filepath) as f:\n",
        "            lines = f.readlines()\n",
        "            file_name_line, num_boxes_line, box_annotation_line = True, False, False\n",
        "            num_boxes, box_counter = 0, 0\n",
        "            labels = []\n",
        "\n",
        "            for line in lines:\n",
        "                line = line.rstrip()\n",
        "                if file_name_line:\n",
        "                    img_path = os.path.join(self.root, \"AUG_WIDER_\" + self.split, \"images\", line)\n",
        "                    img_path = abspath(expanduser(img_path))\n",
        "                    file_name_line, num_boxes_line = False, True\n",
        "                elif num_boxes_line:\n",
        "                    num_boxes = int(line)\n",
        "                    num_boxes_line, box_annotation_line = False, True\n",
        "                elif box_annotation_line:\n",
        "                    box_counter += 1\n",
        "                    line_split = line.split(\" \")\n",
        "                    line_values = [int(x) for x in line_split]\n",
        "                    labels.append(line_values)\n",
        "                    if box_counter >= num_boxes:\n",
        "                        box_annotation_line, file_name_line = False, True\n",
        "                        labels_tensor = torch.tensor(labels)\n",
        "                        cloned_bbox = labels_tensor[:, :4].clone()\n",
        "                        self.img_info.append(\n",
        "                            {\n",
        "                                \"img_path\": img_path,\n",
        "                                \"annotations\": {\n",
        "                                    \"bbox\": cloned_bbox,\n",
        "                                }\n",
        "                            }\n",
        "                        )\n",
        "                        box_counter = 0\n",
        "                        labels.clear()\n",
        "                else:\n",
        "                    raise RuntimeError(f\"Error while parsing annotations file {filepath}\")\n",
        "\n",
        "    def parse_test_annotations_file(self) -> None:\n",
        "        filepath = os.path.join(self.root, \"AUG_wider_face_split\", \"aug_wider_face_test_filelist.txt\")\n",
        "        filepath = abspath(expanduser(filepath))\n",
        "        with open(filepath) as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                line = line.rstrip()\n",
        "                img_path = os.path.join(self.root, \"AUG_WIDER_test\", \"images\", line)\n",
        "                img_path = abspath(expanduser(img_path))\n",
        "                self.img_info.append({\"img_path\": img_path})\n",
        "\n",
        "\n",
        "def parse_target_keys(target_keys, *, available, default):\n",
        "    if target_keys is None:\n",
        "        target_keys = default\n",
        "    if target_keys == \"all\":\n",
        "        target_keys = available\n",
        "    else:\n",
        "        target_keys = set(target_keys)\n",
        "        extra = target_keys - available\n",
        "        if extra:\n",
        "            raise ValueError(f\"Target keys {sorted(extra)} are not available\")\n",
        "\n",
        "    return target_keys\n",
        "\n",
        "\n",
        "@WRAPPER_FACTORIES.register(AugmentedDataset)\n",
        "def ds_wrapper(dataset, target_keys):\n",
        "    target_keys = parse_target_keys(\n",
        "        target_keys,\n",
        "        available={\n",
        "            \"bbox\",\n",
        "        },\n",
        "        default=\"all\",\n",
        "    )\n",
        "\n",
        "    def wrapper(idx, sample):\n",
        "        image, target = sample\n",
        "\n",
        "        if target is None:\n",
        "            return image, target\n",
        "\n",
        "        target = {key: target[key] for key in target_keys}\n",
        "\n",
        "        if \"bbox\" in target_keys:\n",
        "            target[\"bbox\"] = F.convert_bounding_box_format(\n",
        "                tv_tensors.BoundingBoxes(\n",
        "                    target[\"bbox\"], format=tv_tensors.BoundingBoxFormat.XYWH, canvas_size=(image.height, image.width)\n",
        "                ),\n",
        "                new_format=tv_tensors.BoundingBoxFormat.XYXY,\n",
        "            )\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "gMRLfOygY3XH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "3zLgFl_XZ_Jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(imgs, row_title=None, **imshow_kwargs):\n",
        "    if not isinstance(imgs[0], list):\n",
        "        # Make a 2d grid even if there's just 1 row\n",
        "        imgs = [imgs]\n",
        "\n",
        "    num_rows = len(imgs)\n",
        "    num_cols = len(imgs[0])\n",
        "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        for col_idx, img in enumerate(row):\n",
        "            boxes = None\n",
        "            masks = None\n",
        "            if isinstance(img, tuple):\n",
        "                img, target = img\n",
        "                if isinstance(target, dict):\n",
        "                    boxes = target.get(\"bbox\")\n",
        "                    masks = target.get(\"masks\")\n",
        "                elif isinstance(target, tv_tensors.BoundingBoxes):\n",
        "                    boxes = target\n",
        "                else:\n",
        "                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
        "            img = v2.functional.to_image(img)\n",
        "            if img.dtype.is_floating_point and img.min() < 0:\n",
        "                # Poor man's re-normalization for the colors to be OK-ish. This\n",
        "                # is useful for images coming out of Normalize()\n",
        "                img -= img.min()\n",
        "                img /= img.max()\n",
        "\n",
        "            img = v2.functional.to_dtype(img, torch.uint8, scale=True)\n",
        "            if boxes is not None:\n",
        "                img = utils.draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n",
        "            if masks is not None:\n",
        "                img = utils.draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n",
        "\n",
        "            ax = axs[row_idx, col_idx]\n",
        "            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n",
        "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "    if row_title is not None:\n",
        "        for row_idx in range(num_rows):\n",
        "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "Cboe--WfaCWJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_psnr_pt(img, img2, crop_border, test_y_channel=False):\n",
        "    \"\"\"Calculate PSNR (Peak Signal-to-Noise Ratio) (PyTorch version).\n",
        "\n",
        "    Reference: https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\n",
        "\n",
        "    Args:\n",
        "        img (Tensor): Images with range [0, 1], shape (n, 3/1, h, w).\n",
        "        img2 (Tensor): Images with range [0, 1], shape (n, 3/1, h, w).\n",
        "        crop_border (int): Cropped pixels in each edge of an image. These pixels are not involved in the calculation.\n",
        "        test_y_channel (bool): Test on Y channel of YCbCr. Default: False.\n",
        "\n",
        "    Returns:\n",
        "        float: PSNR result.\n",
        "    \"\"\"\n",
        "\n",
        "    assert img.shape == img2.shape, (f'Image shapes are different: {img.shape}, {img2.shape}.')\n",
        "\n",
        "    if crop_border != 0:\n",
        "        img = img[:, :, crop_border:-crop_border, crop_border:-crop_border]\n",
        "        img2 = img2[:, :, crop_border:-crop_border, crop_border:-crop_border]\n",
        "\n",
        "    if test_y_channel:\n",
        "        img = rgb2ycbcr_pt(img, y_only=True)\n",
        "        img2 = rgb2ycbcr_pt(img2, y_only=True)\n",
        "\n",
        "    img = img.to(torch.float64)\n",
        "    img2 = img2.to(torch.float64)\n",
        "\n",
        "    mse = torch.mean((img - img2) ** 2, dim=[1, 2, 3])\n",
        "    return 10. * torch.log10(1. / (mse + 1e-8))\n",
        "\n",
        "\n",
        "def rgb2ycbcr_pt(img, y_only=False):\n",
        "    \"\"\"Convert RGB images to YCbCr images (PyTorch version).\n",
        "\n",
        "    It implements the ITU-R BT.601 conversion for standard-definition television. See more details in\n",
        "    https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.601_conversion.\n",
        "\n",
        "    Args:\n",
        "        img (Tensor): Images with shape (n, 3, h, w), the range [0, 1], float, RGB format.\n",
        "         y_only (bool): Whether to only return Y channel. Default: False.\n",
        "\n",
        "    Returns:\n",
        "        (Tensor): converted images with the shape (n, 3/1, h, w), the range [0, 1], float.\n",
        "    \"\"\"\n",
        "    if y_only:\n",
        "        weight = torch.tensor([[65.481], [128.553], [24.966]]).to(img)\n",
        "        out_img = torch.matmul(img.permute(0, 2, 3, 1), weight).permute(0, 3, 1, 2) + 16.0\n",
        "    else:\n",
        "        weight = torch.tensor([[65.481, -37.797, 112.0], [128.553, -74.203, -93.786], [24.966, 112.0, -18.214]]).to(img)\n",
        "        bias = torch.tensor([16, 128, 128]).view(1, 3, 1, 1).to(img)\n",
        "        out_img = torch.matmul(img.permute(0, 2, 3, 1), weight).permute(0, 3, 1, 2) + bias\n",
        "\n",
        "    out_img = out_img / 255.\n",
        "    return out_img"
      ],
      "metadata": {
        "id": "vN-y5DOqjC4O"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageLoggerMixin:\n",
        "\n",
        "    @overload\n",
        "    def log_images(self, batch: Any, **kwargs: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
        "        ...\n"
      ],
      "metadata": {
        "id": "c8B_1eLMjTIq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageLogger(Callback):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            log_every_n_steps: int = 2000,\n",
        "            max_images_each_step: int = 4,\n",
        "            log_images_kwargs: Dict[str, Any] = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.log_every_n_steps = log_every_n_steps\n",
        "        self.max_images_each_step = max_images_each_step\n",
        "        self.log_images_kwargs = log_images_kwargs or dict()\n",
        "\n",
        "    def on_fit_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:\n",
        "        assert isinstance(pl_module, ImageLoggerMixin)\n",
        "\n",
        "    def on_train_batch_end(\n",
        "            self,\n",
        "            trainer: pl.Trainer,\n",
        "            pl_module: pl.LightningModule,\n",
        "            outputs: STEP_OUTPUT,\n",
        "            batch: Dict[str, torch.Tensor],\n",
        "            batch_idx: int,\n",
        "    ) -> None:\n",
        "        if pl_module.global_step % self.log_every_n_steps == 0:\n",
        "            is_train = pl_module.training\n",
        "            if is_train:\n",
        "                pl_module.freeze()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # returned images should be: nchw, rgb, [0, 1]\n",
        "                images: Dict[str, torch.Tensor] = pl_module.log_images(batch, **self.log_images_kwargs)\n",
        "\n",
        "            # save images\n",
        "            save_dir = os.path.join(pl_module.logger.save_dir, \"image_log\", \"train\")\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "            for image_key in images:\n",
        "                image = images[image_key].detach().cpu()\n",
        "                N = min(self.max_images_each_step, len(image))\n",
        "                grid = torchvision.utils.make_grid(image[:N], nrow=4)\n",
        "                # chw -> hwc (hw if gray)\n",
        "                grid = grid.transpose(0, 1).transpose(1, 2).squeeze(-1).numpy()\n",
        "                grid = (grid * 255).clip(0, 255).astype(np.uint8)\n",
        "                filename = \"{}_step-{:06}_e-{:06}_b-{:06}.png\".format(\n",
        "                    image_key, pl_module.global_step, pl_module.current_epoch, batch_idx\n",
        "                )\n",
        "                path = os.path.join(save_dir, filename)\n",
        "                Image.fromarray(grid).save(path)\n",
        "\n",
        "            if is_train:\n",
        "                pl_module.unfreeze()\n"
      ],
      "metadata": {
        "id": "2IFxfK-8jKyO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "cTprqt7adSyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def window_partition(x, window_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, H, W, C)\n",
        "        window_size (int): window size\n",
        "\n",
        "    Returns:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "    \"\"\"\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows\n",
        "\n",
        "\n",
        "def window_reverse(windows, window_size, H, W):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "        window_size (int): Window size\n",
        "        H (int): Height of image\n",
        "        W (int): Width of image\n",
        "\n",
        "    Returns:\n",
        "        x: (B, H, W, C)\n",
        "    \"\"\"\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return x\n",
        "\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
        "    It supports both of shifted and non-shifted window.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        window_size (tuple[int]): The height and width of the window.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
        "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
        "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size  # Wh, Ww\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        # define a parameter table of relative position bias\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "        # get pair-wise relative position index for each token inside the window\n",
        "        coords_h = torch.arange(self.window_size[0])\n",
        "        coords_w = torch.arange(self.window_size[1])\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape of (num_windows*B, N, C)\n",
        "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
        "        \"\"\"\n",
        "        B_, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))\n",
        "\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = self.softmax(attn)\n",
        "        else:\n",
        "            attn = self.softmax(attn)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
        "\n",
        "    def flops(self, N):\n",
        "        # calculate flops for 1 window with token length of N\n",
        "        flops = 0\n",
        "        # qkv = self.qkv(x)\n",
        "        flops += N * self.dim * 3 * self.dim\n",
        "        # attn = (q @ k.transpose(-2, -1))\n",
        "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
        "        #  x = (attn @ v)\n",
        "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
        "        # x = self.proj(x)\n",
        "        flops += N * self.dim * self.dim\n",
        "        return flops\n",
        "\n",
        "\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    r\"\"\" Swin Transformer Block.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        input_resolution (tuple[int]): Input resulotion.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        window_size (int): Window size.\n",
        "        shift_size (int): Shift size for SW-MSA.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
        "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        if min(self.input_resolution) <= self.window_size:\n",
        "            # if window size is larger than input resolution, we don't partition windows\n",
        "            self.shift_size = 0\n",
        "            self.window_size = min(self.input_resolution)\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
        "\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = WindowAttention(\n",
        "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            attn_mask = self.calculate_mask(self.input_resolution)\n",
        "        else:\n",
        "            attn_mask = None\n",
        "\n",
        "        self.register_buffer(\"attn_mask\", attn_mask)\n",
        "\n",
        "    def calculate_mask(self, x_size):\n",
        "        # calculate attention mask for SW-MSA\n",
        "        H, W = x_size\n",
        "        img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
        "        h_slices = (slice(0, -self.window_size),\n",
        "                    slice(-self.window_size, -self.shift_size),\n",
        "                    slice(-self.shift_size, None))\n",
        "        w_slices = (slice(0, -self.window_size),\n",
        "                    slice(-self.window_size, -self.shift_size),\n",
        "                    slice(-self.shift_size, None))\n",
        "        cnt = 0\n",
        "        for h in h_slices:\n",
        "            for w in w_slices:\n",
        "                img_mask[:, h, w, :] = cnt\n",
        "                cnt += 1\n",
        "\n",
        "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
        "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
        "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
        "\n",
        "        return attn_mask\n",
        "\n",
        "    def forward(self, x, x_size):\n",
        "        H, W = x_size\n",
        "        B, L, C = x.shape\n",
        "        # assert L == H * W, \"input feature has wrong size\"\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_x = x\n",
        "\n",
        "        # partition windows\n",
        "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
        "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
        "\n",
        "        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n",
        "        if self.input_resolution == x_size:\n",
        "            attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
        "        else:\n",
        "            attn_windows = self.attn(x_windows, mask=self.calculate_mask(x_size).to(x.device))\n",
        "\n",
        "        # merge windows\n",
        "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
        "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
        "\n",
        "        # reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "        x = x.view(B, H * W, C)\n",
        "\n",
        "        # FFN\n",
        "        x = shortcut + self.drop_path(x)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
        "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        H, W = self.input_resolution\n",
        "        # norm1\n",
        "        flops += self.dim * H * W\n",
        "        # W-MSA/SW-MSA\n",
        "        nW = H * W / self.window_size / self.window_size\n",
        "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
        "        # mlp\n",
        "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
        "        # norm2\n",
        "        flops += self.dim * H * W\n",
        "        return flops\n",
        "\n",
        "\n",
        "class PatchMerging(nn.Module):\n",
        "    r\"\"\" Patch Merging Layer.\n",
        "\n",
        "    Args:\n",
        "        input_resolution (tuple[int]): Resolution of input feature.\n",
        "        dim (int): Number of input channels.\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.input_resolution = input_resolution\n",
        "        self.dim = dim\n",
        "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
        "        self.norm = norm_layer(4 * dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: B, H*W, C\n",
        "        \"\"\"\n",
        "        H, W = self.input_resolution\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"input feature has wrong size\"\n",
        "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
        "\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
        "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
        "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
        "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
        "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
        "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
        "\n",
        "    def flops(self):\n",
        "        H, W = self.input_resolution\n",
        "        flops = H * W * self.dim\n",
        "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
        "        return flops\n",
        "\n",
        "\n",
        "class BasicLayer(nn.Module):\n",
        "    \"\"\" A basic Swin Transformer layer for one stage.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        input_resolution (tuple[int]): Input resolution.\n",
        "        depth (int): Number of blocks.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        window_size (int): Local window size.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.depth = depth\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "        # build blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
        "                                 num_heads=num_heads, window_size=window_size,\n",
        "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
        "                                 mlp_ratio=mlp_ratio,\n",
        "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                 drop=drop, attn_drop=attn_drop,\n",
        "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                                 norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "\n",
        "        # patch merging layer\n",
        "        if downsample is not None:\n",
        "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "    def forward(self, x, x_size):\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x, x_size)\n",
        "            else:\n",
        "                x = blk(x, x_size)\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        for blk in self.blocks:\n",
        "            flops += blk.flops()\n",
        "        if self.downsample is not None:\n",
        "            flops += self.downsample.flops()\n",
        "        return flops\n",
        "\n",
        "\n",
        "class RSTB(nn.Module):\n",
        "    \"\"\"Residual Swin Transformer Block (RSTB).\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        input_resolution (tuple[int]): Input resolution.\n",
        "        depth (int): Number of blocks.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        window_size (int): Local window size.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
        "        img_size: Input image size.\n",
        "        patch_size: Patch size.\n",
        "        resi_connection: The convolutional block before residual connection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n",
        "                 img_size=224, patch_size=4, resi_connection='1conv'):\n",
        "        super(RSTB, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "\n",
        "        self.residual_group = BasicLayer(dim=dim,\n",
        "                                         input_resolution=input_resolution,\n",
        "                                         depth=depth,\n",
        "                                         num_heads=num_heads,\n",
        "                                         window_size=window_size,\n",
        "                                         mlp_ratio=mlp_ratio,\n",
        "                                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                         drop=drop, attn_drop=attn_drop,\n",
        "                                         drop_path=drop_path,\n",
        "                                         norm_layer=norm_layer,\n",
        "                                         downsample=downsample,\n",
        "                                         use_checkpoint=use_checkpoint)\n",
        "\n",
        "        if resi_connection == '1conv':\n",
        "            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n",
        "        elif resi_connection == '3conv':\n",
        "            # to save parameters and memory\n",
        "            self.conv = nn.Sequential(nn.Conv2d(dim, dim // 4, 3, 1, 1), nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "                                      nn.Conv2d(dim // 4, dim // 4, 1, 1, 0),\n",
        "                                      nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "                                      nn.Conv2d(dim // 4, dim, 3, 1, 1))\n",
        "\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n",
        "            norm_layer=None)\n",
        "\n",
        "        self.patch_unembed = PatchUnEmbed(\n",
        "            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n",
        "            norm_layer=None)\n",
        "\n",
        "    def forward(self, x, x_size):\n",
        "        return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        flops += self.residual_group.flops()\n",
        "        H, W = self.input_resolution\n",
        "        flops += H * W * self.dim * self.dim * 9\n",
        "        flops += self.patch_embed.flops()\n",
        "        flops += self.patch_unembed.flops()\n",
        "\n",
        "        return flops\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    r\"\"\" Image to Patch Embedding\n",
        "\n",
        "    Args:\n",
        "        img_size (int): Image size.  Default: 224.\n",
        "        patch_size (int): Patch token size. Default: 4.\n",
        "        in_chans (int): Number of input image channels. Default: 3.\n",
        "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.patches_resolution = patches_resolution\n",
        "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
        "\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        if norm_layer is not None:\n",
        "            self.norm = norm_layer(embed_dim)\n",
        "        else:\n",
        "            self.norm = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        H, W = self.img_size\n",
        "        if self.norm is not None:\n",
        "            flops += H * W * self.embed_dim\n",
        "        return flops\n",
        "\n",
        "\n",
        "class PatchUnEmbed(nn.Module):\n",
        "    r\"\"\" Image to Patch Unembedding\n",
        "\n",
        "    Args:\n",
        "        img_size (int): Image size.  Default: 224.\n",
        "        patch_size (int): Patch token size. Default: 4.\n",
        "        in_chans (int): Number of input image channels. Default: 3.\n",
        "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.patches_resolution = patches_resolution\n",
        "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
        "\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, x, x_size):\n",
        "        B, HW, C = x.shape\n",
        "        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C\n",
        "        return x\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        return flops\n",
        "\n",
        "\n",
        "class Upsample(nn.Sequential):\n",
        "    \"\"\"Upsample module.\n",
        "\n",
        "    Args:\n",
        "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
        "        num_feat (int): Channel number of intermediate features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scale, num_feat):\n",
        "        m = []\n",
        "        if (scale & (scale - 1)) == 0:  # scale = 2^n\n",
        "            for _ in range(int(math.log(scale, 2))):\n",
        "                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n",
        "                m.append(nn.PixelShuffle(2))\n",
        "        elif scale == 3:\n",
        "            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n",
        "            m.append(nn.PixelShuffle(3))\n",
        "        else:\n",
        "            raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')\n",
        "        super(Upsample, self).__init__(*m)\n",
        "\n",
        "\n",
        "class UpsampleOneStep(nn.Sequential):\n",
        "    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n",
        "       Used in lightweight SR to save parameters.\n",
        "\n",
        "    Args:\n",
        "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
        "        num_feat (int): Channel number of intermediate features.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n",
        "        self.num_feat = num_feat\n",
        "        self.input_resolution = input_resolution\n",
        "        m = []\n",
        "        m.append(nn.Conv2d(num_feat, (scale ** 2) * num_out_ch, 3, 1, 1))\n",
        "        m.append(nn.PixelShuffle(scale))\n",
        "        super(UpsampleOneStep, self).__init__(*m)\n",
        "\n",
        "    def flops(self):\n",
        "        H, W = self.input_resolution\n",
        "        flops = H * W * self.num_feat * 3 * 9\n",
        "        return flops\n",
        "\n",
        "\n",
        "class SwinIR(pl.LightningModule, ImageLoggerMixin):\n",
        "    r\"\"\" SwinIR\n",
        "        A PyTorch impl of : `SwinIR: Image Restoration Using Swin Transformer`, based on Swin Transformer.\n",
        "\n",
        "    Args:\n",
        "        img_size (int | tuple(int)): Input image size. Default 64\n",
        "        patch_size (int | tuple(int)): Patch size. Default: 1\n",
        "        in_chans (int): Number of input image channels. Default: 3\n",
        "        embed_dim (int): Patch embedding dimension. Default: 96\n",
        "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
        "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
        "        window_size (int): Window size. Default: 7\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
        "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
        "        drop_rate (float): Dropout rate. Default: 0\n",
        "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
        "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
        "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
        "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
        "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
        "        sf: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n",
        "        img_range: Image range. 1. or 255.\n",
        "        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n",
        "        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=64,\n",
        "        patch_size=1,\n",
        "        in_chans=3,\n",
        "        embed_dim=96,\n",
        "        depths=[6, 6, 6, 6],\n",
        "        num_heads=[6, 6, 6, 6],\n",
        "        window_size=7,\n",
        "        mlp_ratio=4.,\n",
        "        qkv_bias=True,\n",
        "        qk_scale=None,\n",
        "        drop_rate=0.,\n",
        "        attn_drop_rate=0.,\n",
        "        drop_path_rate=0.1,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        ape=False,\n",
        "        patch_norm=True,\n",
        "        use_checkpoint=False,\n",
        "        sf=4,\n",
        "        img_range=1.,\n",
        "        upsampler='',\n",
        "        resi_connection='1conv',\n",
        "        unshuffle=False,\n",
        "        unshuffle_scale=None,\n",
        "        hq_key: str=\"jpg\",\n",
        "        lq_key: str=\"hint\",\n",
        "        learning_rate: float=None,\n",
        "        weight_decay: float=None\n",
        "    ) -> \"SwinIR\":\n",
        "        super(SwinIR, self).__init__()\n",
        "        num_in_ch = in_chans * (unshuffle_scale**2) if unshuffle else in_chans\n",
        "        num_out_ch = in_chans\n",
        "        num_feat = 64\n",
        "        self.img_range = img_range\n",
        "        if in_chans == 3:\n",
        "            rgb_mean = (0.4488, 0.4371, 0.4040)\n",
        "            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n",
        "        else:\n",
        "            self.mean = torch.zeros(1, 1, 1, 1)\n",
        "        self.upscale = sf\n",
        "        self.upsampler = upsampler\n",
        "        self.window_size = window_size\n",
        "        self.unshuffle_scale = unshuffle_scale\n",
        "        self.unshuffle = unshuffle\n",
        "\n",
        "        #####################################################################################################\n",
        "        ################################### 1, shallow feature extraction ###################################\n",
        "        if unshuffle:\n",
        "            assert unshuffle_scale is not None\n",
        "            self.conv_first = nn.Sequential(\n",
        "                nn.PixelUnshuffle(sf),\n",
        "                nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1),\n",
        "            )\n",
        "        else:\n",
        "            self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n",
        "\n",
        "        #####################################################################################################\n",
        "        ################################### 2, deep feature extraction ######################################\n",
        "        self.num_layers = len(depths)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ape = ape\n",
        "        self.patch_norm = patch_norm\n",
        "        self.num_features = embed_dim\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        # split image into non-overlapping patches\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n",
        "            norm_layer=norm_layer if self.patch_norm else None\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        patches_resolution = self.patch_embed.patches_resolution\n",
        "        self.patches_resolution = patches_resolution\n",
        "\n",
        "        # merge non-overlapping patches into image\n",
        "        self.patch_unembed = PatchUnEmbed(\n",
        "            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n",
        "            norm_layer=norm_layer if self.patch_norm else None\n",
        "        )\n",
        "\n",
        "        # absolute position embedding\n",
        "        if self.ape:\n",
        "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
        "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
        "\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # stochastic depth\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
        "\n",
        "        # build Residual Swin Transformer blocks (RSTB)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i_layer in range(self.num_layers):\n",
        "            layer = RSTB(\n",
        "                dim=embed_dim,\n",
        "                input_resolution=(patches_resolution[0], patches_resolution[1]),\n",
        "                depth=depths[i_layer],\n",
        "                num_heads=num_heads[i_layer],\n",
        "                window_size=window_size,\n",
        "                mlp_ratio=self.mlp_ratio,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results\n",
        "                norm_layer=norm_layer,\n",
        "                downsample=None,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                img_size=img_size,\n",
        "                patch_size=patch_size,\n",
        "                resi_connection=resi_connection\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "        self.norm = norm_layer(self.num_features)\n",
        "\n",
        "        # build the last conv layer in deep feature extraction\n",
        "        if resi_connection == '1conv':\n",
        "            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n",
        "        elif resi_connection == '3conv':\n",
        "            # to save parameters and memory\n",
        "            self.conv_after_body = nn.Sequential(\n",
        "                nn.Conv2d(embed_dim, embed_dim // 4, 3, 1, 1),\n",
        "                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "                nn.Conv2d(embed_dim // 4, embed_dim // 4, 1, 1, 0),\n",
        "                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "                nn.Conv2d(embed_dim // 4, embed_dim, 3, 1, 1)\n",
        "            )\n",
        "\n",
        "        #####################################################################################################\n",
        "        ################################ 3, high quality image reconstruction ################################\n",
        "        if self.upsampler == 'pixelshuffle':\n",
        "            # for classical SR\n",
        "            self.conv_before_upsample = nn.Sequential(\n",
        "                nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n",
        "                nn.LeakyReLU(inplace=True)\n",
        "            )\n",
        "            self.upsample = Upsample(sf, num_feat)\n",
        "            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
        "        elif self.upsampler == 'pixelshuffledirect':\n",
        "            # for lightweight SR (to save parameters)\n",
        "            self.upsample = UpsampleOneStep(\n",
        "                sf, embed_dim, num_out_ch,\n",
        "                (patches_resolution[0], patches_resolution[1])\n",
        "            )\n",
        "        elif self.upsampler == 'nearest+conv':\n",
        "            # for real-world SR (less artifacts)\n",
        "            self.conv_before_upsample = nn.Sequential(\n",
        "                nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n",
        "                nn.LeakyReLU(inplace=True)\n",
        "            )\n",
        "            self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
        "            if self.upscale == 4:\n",
        "                self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
        "            elif self.upscale == 8:\n",
        "                self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
        "                self.conv_up3 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
        "            self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
        "            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
        "            self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        else:\n",
        "            # for image denoising and JPEG compression artifact reduction\n",
        "            self.conv_last = nn.Conv2d(embed_dim, num_out_ch, 3, 1, 1)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        self.hq_key = hq_key\n",
        "        self.lq_key = lq_key\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.lpips_metric = LPIPS(net=\"alex\")\n",
        "\n",
        "    def _init_weights(self, m: nn.Module) -> None:\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    # TODO: What's this ?\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self) -> Set[str]:\n",
        "        return {'absolute_pos_embed'}\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self) -> Set[str]:\n",
        "        return {'relative_position_bias_table'}\n",
        "\n",
        "    def check_image_size(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        _, _, h, w = x.size()\n",
        "        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n",
        "        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n",
        "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
        "        return x\n",
        "\n",
        "    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x_size = (x.shape[2], x.shape[3])\n",
        "        x = self.patch_embed(x)\n",
        "        if self.ape:\n",
        "            x = x + self.absolute_pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, x_size)\n",
        "\n",
        "        x = self.norm(x)  # B L C\n",
        "        x = self.patch_unembed(x, x_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        H, W = x.shape[2:]\n",
        "        x = self.check_image_size(x)\n",
        "\n",
        "        self.mean = self.mean.type_as(x)\n",
        "        x = (x - self.mean) * self.img_range\n",
        "\n",
        "        if self.upsampler == 'pixelshuffle':\n",
        "            # for classical SR\n",
        "            x = self.conv_first(x)\n",
        "            x = self.conv_after_body(self.forward_features(x)) + x\n",
        "            x = self.conv_before_upsample(x)\n",
        "            x = self.conv_last(self.upsample(x))\n",
        "        elif self.upsampler == 'pixelshuffledirect':\n",
        "            # for lightweight SR\n",
        "            x = self.conv_first(x)\n",
        "            x = self.conv_after_body(self.forward_features(x)) + x\n",
        "            x = self.upsample(x)\n",
        "        elif self.upsampler == 'nearest+conv':\n",
        "            # for real-world SR\n",
        "            x = self.conv_first(x)\n",
        "            x = self.conv_after_body(self.forward_features(x)) + x\n",
        "            x = self.conv_before_upsample(x)\n",
        "            x = self.lrelu(self.conv_up1(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n",
        "            if self.upscale == 4:\n",
        "                x = self.lrelu(self.conv_up2(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n",
        "            elif self.upscale == 8:\n",
        "                x = self.lrelu(self.conv_up2(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n",
        "                x = self.lrelu(self.conv_up3(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n",
        "            x = self.conv_last(self.lrelu(self.conv_hr(x)))\n",
        "        else:\n",
        "            # for image denoising and JPEG compression artifact reduction\n",
        "            x_first = self.conv_first(x)\n",
        "            res = self.conv_after_body(self.forward_features(x_first)) + x_first\n",
        "            x = x + self.conv_last(res)\n",
        "\n",
        "        x = x / self.img_range + self.mean\n",
        "\n",
        "        return x[:, :, :H*self.upscale, :W*self.upscale]\n",
        "\n",
        "    def flops(self) -> int:\n",
        "        flops = 0\n",
        "        H, W = self.patches_resolution\n",
        "        flops += H * W * 3 * self.embed_dim * 9\n",
        "        flops += self.patch_embed.flops()\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            flops += layer.flops()\n",
        "        flops += H * W * 3 * self.embed_dim * self.embed_dim\n",
        "        flops += self.upsample.flops()\n",
        "        return flops\n",
        "\n",
        "    def get_loss(self, pred: torch.Tensor, label: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute loss between model predictions and labels.\n",
        "\n",
        "        Args:\n",
        "            pred (torch.Tensor): Batch model predictions.\n",
        "            label (torch.Tensor): Batch labels.\n",
        "\n",
        "        Returns:\n",
        "            loss (torch.Tensor): The loss tensor.\n",
        "        \"\"\"\n",
        "        return mse_loss(input=pred, target=label, reduction=\"sum\")\n",
        "\n",
        "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> STEP_OUTPUT:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            batch (Dict[str, torch.Tensor]): A dict contains LQ and HQ (NHWC, RGB,\n",
        "                LQ range in [0, 1] and HQ range in [-1, 1]).\n",
        "            batch_idx (int): Index of this batch.\n",
        "\n",
        "        Returns:\n",
        "            outputs (torch.Tensor): The loss tensor.\n",
        "        \"\"\"\n",
        "        hq, lq = batch[self.hq_key], batch[self.lq_key]\n",
        "        hq = rearrange(((hq + 1) / 2).clamp_(0, 1), \"n h w c -> n c h w\")\n",
        "        lq = rearrange(lq, \"n h w c -> n c h w\")\n",
        "        pred = self(lq)\n",
        "        loss = self.get_loss(pred, hq)\n",
        "        self.log(\"train_loss\", loss, on_step=True)\n",
        "        return loss\n",
        "\n",
        "    def on_validation_start(self) -> None:\n",
        "        self.lpips_metric.to(self.device)\n",
        "\n",
        "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> None:\n",
        "        hq, lq = batch[self.hq_key], batch[self.lq_key]\n",
        "        lq = rearrange(lq, \"n h w c -> n c h w\")\n",
        "        pred = self(lq)\n",
        "        hq = rearrange(((hq + 1) / 2).clamp_(0, 1), \"n h w c -> n c h w\")\n",
        "\n",
        "        # requiremtns for lpips model inputs:\n",
        "        # https://github.com/richzhang/PerceptualSimilarity\n",
        "        lpips = self.lpips_metric(pred, hq, normalize=True).mean()\n",
        "        self.log(\"val_lpips\", lpips)\n",
        "\n",
        "        pnsr = calculate_psnr_pt(pred, hq, crop_border=0).mean()\n",
        "        self.log(\"val_pnsr\", pnsr)\n",
        "\n",
        "        loss = self.get_loss(pred, hq)\n",
        "        self.log(\"val_loss\", loss)\n",
        "\n",
        "    def configure_optimizers(self) -> optim.AdamW:\n",
        "        \"\"\"\n",
        "        Configure optimizer for this model.\n",
        "\n",
        "        Returns:\n",
        "            optimizer (optim.AdamW): The optimizer for this model.\n",
        "        \"\"\"\n",
        "        optimizer = optim.AdamW(\n",
        "            [p for p in self.parameters() if p.requires_grad], lr=self.learning_rate,\n",
        "            weight_decay=self.weight_decay\n",
        "        )\n",
        "        return optimizer\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def log_images(self, batch: Any) -> Dict[str, torch.Tensor]:\n",
        "        hq, lq = batch[self.hq_key], batch[self.lq_key]\n",
        "        hq = rearrange(((hq + 1) / 2).clamp_(0, 1), \"n h w c -> n c h w\")\n",
        "        lq = rearrange(lq, \"n h w c -> n c h w\")\n",
        "        pred = self(lq)\n",
        "        return dict(lq=lq, pred=pred, hq=hq)"
      ],
      "metadata": {
        "id": "vRLMGDH0dRkA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "p_1E7m4YZSB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = pathlib.Path('.')"
      ],
      "metadata": {
        "id": "zUpLVEMrZQye"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_ds = AugmentedDataset(ROOT, 'train')\n",
        "aug_ds = wrap_dataset_for_transforms_v2(aug_ds)"
      ],
      "metadata": {
        "id": "WdZ8SzkdZfqk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DiffBIR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oxhtYhjmUEV",
        "outputId": "506c427a-0475-4a83-8a08-580f79f7002d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DiffBIR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/inference_stage1.py --config configs/model/swinir.yaml --ckpt weights/face_swinir_v1.ckpt --input ../aug_widerface/AUG_WIDER_val/images/0--Parade --sr_scale 1 --image_size 512 --output ../output/face-restore/AUG_WIDER_val/images/0--Parade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTGZJhPvZ630",
        "outputId": "ca90289b-c152-4261-b15e-ba49bc69095c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed set to 231\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100% 233M/233M [00:02<00:00, 81.8MB/s]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_12_aug_7.jpg: 100% 1035/1035 [09:49<00:00,  1.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/inference_stage1.py --config configs/model/swinir.yaml --ckpt weights/general_swinir_v1.ckpt --input ../aug_widerface/AUG_WIDER_val/images/0--Parade --sr_scale 1 --image_size 512 --output ../output/general-restore/AUG_WIDER_val/images/0--Parade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM2_qiNOn6YL",
        "outputId": "9f72ac9e-8491-4ab2-c6c1-3ccf80347762"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed set to 231\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_12_aug_7.jpg: 100% 1035/1035 [09:36<00:00,  1.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/inference_stage1.py --config configs/model/swinir.yaml --ckpt weights/face_swinir_v1.ckpt --input ../aug_widerface/AUG_WIDER_val/images/0--Parade --sr_scale 2 --image_size 512 --output ../output/face-restore-x2/AUG_WIDER_val/images/0--Parade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNdPZ5J1rYWr",
        "outputId": "e68109c9-556f-498c-8b19-27a3701f6d2d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed set to 231\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_12_aug_7.jpg: 100% 1035/1035 [35:24<00:00,  2.05s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/inference_stage1.py --config configs/model/swinir.yaml --ckpt weights/face_swinir_v1.ckpt --input ../aug_widerface/AUG_WIDER_val/images/0--Parade --sr_scale 4 --image_size 512 --output ../output/face-restore-x4/AUG_WIDER_val/images/0--Parade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTim8rr4w5Xk",
        "outputId": "1caf6aaa-e541-40ff-8ec6-d713b5a9670a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed set to 231\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_194_aug_5.jpg:   5% 49/1035 [07:30<2:57:56, 10.83s/it]      inference failed, error: CUDA out of memory. Tried to allocate 5.44 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.62 GiB is free. Process 396210 has 11.15 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_829_aug_2.jpg:   6% 57/1035 [09:02<2:56:54, 10.85s/it]      inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.62 GiB is free. Process 396210 has 11.15 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 3.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_468_aug_1.jpg:   7% 71/1035 [11:20<2:36:11,  9.72s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.06 GiB is free. Process 396210 has 11.71 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_4.jpg:   9% 89/1035 [14:19<2:19:48,  8.87s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.00 GiB is free. Process 396210 has 11.77 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 3.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_818_aug_4.jpg:  12% 122/1035 [19:18<2:12:35,  8.71s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.00 GiB is free. Process 396210 has 11.77 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_246.jpg:  12% 124/1035 [19:44<2:42:58, 10.73s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.00 GiB is free. Process 396210 has 11.77 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 6.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_459_aug_1.jpg:  12% 127/1035 [20:06<2:01:39,  8.04s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.94 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.00 GiB is free. Process 396210 has 11.77 GiB memory in use. Of the allocated memory 6.50 GiB is allocated by PyTorch, and 4.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_5.jpg:  14% 143/1035 [22:31<1:30:42,  6.10s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.00 GiB is free. Process 396210 has 11.77 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 3.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_829_aug_1.jpg:  15% 151/1035 [23:53<2:24:30,  9.81s/it]      inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.00 GiB is free. Process 396210 has 11.77 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_356_aug_3.jpg:  15% 155/1035 [24:44<2:48:56, 11.52s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_468_aug_4.jpg:  16% 168/1035 [26:47<2:10:09,  9.01s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_829_aug_7.jpg:  17% 177/1035 [27:59<1:48:46,  7.61s/it]      inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_468.jpg:  20% 202/1035 [31:54<2:15:47,  9.78s/it]      inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_829_aug_3.jpg:  21% 217/1035 [34:06<2:04:21,  9.12s/it]      inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_818.jpg:  22% 228/1035 [35:48<2:03:30,  9.18s/it]      inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353_aug_2.jpg:  23% 239/1035 [37:46<2:14:10, 10.11s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_194_aug_7.jpg:  23% 242/1035 [38:23<2:26:54, 11.12s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.44 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 4.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353_aug_7.jpg:  26% 274/1035 [42:56<1:49:47,  8.66s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_356_aug_1.jpg:  27% 278/1035 [43:40<1:56:23,  9.23s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_2.jpg:  29% 302/1035 [47:40<2:16:04, 11.14s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 3.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_6.jpg:  29% 303/1035 [47:56<2:34:29, 12.66s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 3.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_194_aug_3.jpg:  32% 335/1035 [52:24<1:20:58,  6.94s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.44 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 4.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_194.jpg:  33% 342/1035 [53:34<1:59:04, 10.31s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.44 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 4.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_468_aug_3.jpg:  38% 395/1035 [1:01:16<1:21:58,  7.69s/it]      inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_246_aug_3.jpg:  39% 407/1035 [1:03:21<1:16:27,  7.31s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 6.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_246_aug_0.jpg:  40% 416/1035 [1:04:39<1:39:56,  9.69s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 6.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561.jpg:  41% 428/1035 [1:06:12<1:15:58,  7.51s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 3.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353_aug_6.jpg:  42% 437/1035 [1:07:38<1:21:25,  8.17s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_356.jpg:  44% 459/1035 [1:10:44<1:14:34,  7.77s/it]      inference failed, error: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_818_aug_7.jpg:  45% 461/1035 [1:11:07<1:27:31,  9.15s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_818_aug_1.jpg:  48% 499/1035 [1:17:05<1:15:24,  8.44s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_194_aug_0.jpg:  49% 508/1035 [1:18:33<1:04:15,  7.32s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.44 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 4.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_356_aug_6.jpg:  49% 511/1035 [1:19:02<1:15:24,  8.64s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_246_aug_6.jpg:  50% 520/1035 [1:20:23<1:04:21,  7.50s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 6.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353.jpg:  51% 533/1035 [1:22:21<1:20:53,  9.67s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_545_aug_5.jpg:  55% 567/1035 [1:27:27<1:08:39,  8.80s/it]      inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 6.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353_aug_5.jpg:  56% 578/1035 [1:29:06<50:34,  6.64s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_468_aug_0.jpg:  57% 585/1035 [1:30:08<56:59,  7.60s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_468_aug_5.jpg:  58% 603/1035 [1:32:52<39:41,  5.51s/it]      inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_901_aug_7.jpg:  59% 611/1035 [1:34:12<47:44,  6.75s/it]      inference failed, error: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 6.09 GiB is allocated by PyTorch, and 5.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_194_aug_6.jpg:  60% 616/1035 [1:34:56<48:33,  6.95s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.44 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 4.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_818_aug_3.jpg:  60% 619/1035 [1:35:26<54:42,  7.89s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 396210 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353_aug_4.jpg:  72% 742/1035 [1:53:36<52:43, 10.80s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 5.06 GiB is free. Process 396210 has 10.71 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 3.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_818_aug_2.jpg:  76% 783/1035 [1:59:40<35:40,  8.49s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 5.06 GiB is free. Process 396210 has 10.71 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 2.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_356_aug_4.jpg:  76% 786/1035 [2:00:11<36:07,  8.71s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 4.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_829_aug_6.jpg:  78% 810/1035 [2:03:39<25:07,  6.70s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 6.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_356_aug_0.jpg:  79% 814/1035 [2:04:20<33:34,  9.12s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 4.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_545_aug_3.jpg:  82% 846/1035 [2:09:27<25:30,  8.10s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 7.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_545_aug_2.jpg:  82% 847/1035 [2:09:38<28:07,  8.97s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 7.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_194_aug_4.jpg:  84% 867/1035 [2:12:08<20:47,  7.43s/it]      inference failed, error: CUDA out of memory. Tried to allocate 5.44 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 6.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_901_aug_1.jpg:  84% 870/1035 [2:12:37<21:56,  7.98s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 6.09 GiB is allocated by PyTorch, and 7.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_545_aug_0.jpg:  84% 871/1035 [2:12:49<24:59,  9.15s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 7.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_356_aug_5.jpg:  86% 889/1035 [2:15:19<19:29,  8.01s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 4.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_818_aug_5.jpg:  86% 892/1035 [2:15:56<24:52, 10.44s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 5.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_829_aug_4.jpg:  87% 896/1035 [2:16:44<25:54, 11.19s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 6.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_3.jpg:  87% 899/1035 [2:17:18<24:53, 10.98s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 5.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_246_aug_2.jpg:  90% 930/1035 [2:21:44<17:18,  9.89s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 7.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353_aug_1.jpg:  91% 944/1035 [2:24:13<14:00,  9.24s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 6.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_459.jpg:  92% 952/1035 [2:25:16<09:39,  6.98s/it]     inference failed, error: CUDA out of memory. Tried to allocate 4.94 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 6.50 GiB is allocated by PyTorch, and 6.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_1.jpg:  94% 968/1035 [2:27:43<08:47,  7.88s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 5.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_0.jpg:  95% 987/1035 [2:30:30<05:37,  7.03s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 5.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_468_aug_6.jpg:  97% 1007/1035 [2:33:33<03:33,  7.63s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 5.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_545.jpg:  98% 1013/1035 [2:34:14<01:56,  5.29s/it]           inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 7.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_7.jpg:  98% 1014/1035 [2:34:22<02:12,  6.32s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 5.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353_aug_0.jpg:  99% 1024/1035 [2:35:54<01:50, 10.06s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 6.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_829.jpg: 100% 1031/1035 [2:36:53<00:37,  9.40s/it]      inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 396210 has 13.52 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 6.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_12_aug_7.jpg: 100% 1035/1035 [2:37:23<00:00,  9.12s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/inference_stage1.py --config configs/model/swinir.yaml --ckpt weights/general_swinir_v1.ckpt --input ../aug_widerface/AUG_WIDER_val/images/0--Parade --sr_scale 2 --image_size 512 --output ../output/general-restore-x2/AUG_WIDER_val/images/0--Parade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWoba3J2w8NR",
        "outputId": "6e662e47-647b-41e4-c0e6-56911bb45284"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed set to 231\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_12_aug_7.jpg: 100% 1035/1035 [33:06<00:00,  1.92s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/inference_stage1.py --config configs/model/swinir.yaml --ckpt weights/general_swinir_v1.ckpt --input ../aug_widerface/AUG_WIDER_val/images/0--Parade --sr_scale 4 --image_size 512 --output ../output/general-restore-x4/AUG_WIDER_val/images/0--Parade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kw52vGprw-la",
        "outputId": "2bd0d55e-7fd0-4344-cd99-27008c42353b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed set to 231\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_194_aug_5.jpg:   5% 49/1035 [07:15<2:53:17, 10.55s/it]      inference failed, error: CUDA out of memory. Tried to allocate 5.44 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.62 GiB is free. Process 1570959 has 11.15 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_829_aug_2.jpg:   6% 57/1035 [08:38<2:37:24,  9.66s/it]      inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.62 GiB is free. Process 1570959 has 11.15 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 3.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_468_aug_1.jpg:   7% 71/1035 [10:52<2:25:58,  9.09s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.06 GiB is free. Process 1570959 has 11.71 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_4.jpg:   9% 89/1035 [13:47<2:13:53,  8.49s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.00 GiB is free. Process 1570959 has 11.77 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 3.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_818_aug_4.jpg:  12% 122/1035 [19:05<2:22:33,  9.37s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.00 GiB is free. Process 1570959 has 11.77 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_246.jpg:  12% 124/1035 [19:33<2:54:55, 11.52s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.00 GiB is free. Process 1570959 has 11.77 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 6.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_459_aug_1.jpg:  12% 127/1035 [19:54<2:02:38,  8.10s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.94 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.00 GiB is free. Process 1570959 has 11.77 GiB memory in use. Of the allocated memory 6.50 GiB is allocated by PyTorch, and 4.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_5.jpg:  14% 143/1035 [22:13<1:28:16,  5.94s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.00 GiB is free. Process 1570959 has 11.77 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 3.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_829_aug_1.jpg:  15% 151/1035 [23:33<2:23:38,  9.75s/it]      inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 4.00 GiB is free. Process 1570959 has 11.77 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_356_aug_3.jpg:  15% 155/1035 [24:25<2:51:14, 11.68s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_468_aug_4.jpg:  16% 168/1035 [26:24<2:09:28,  8.96s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_829_aug_7.jpg:  17% 177/1035 [27:32<1:41:44,  7.11s/it]      inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_468.jpg:  20% 202/1035 [31:18<2:10:29,  9.40s/it]      inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_829_aug_3.jpg:  21% 217/1035 [33:20<1:56:41,  8.56s/it]      inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_818.jpg:  22% 228/1035 [34:57<1:57:15,  8.72s/it]      inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353_aug_2.jpg:  23% 239/1035 [36:50<2:12:56, 10.02s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_194_aug_7.jpg:  23% 242/1035 [37:25<2:24:18, 10.92s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.44 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 4.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353_aug_7.jpg:  26% 274/1035 [41:44<1:48:25,  8.55s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_356_aug_1.jpg:  27% 278/1035 [42:24<1:46:19,  8.43s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_2.jpg:  29% 302/1035 [46:12<2:07:15, 10.42s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 3.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_6.jpg:  29% 303/1035 [46:28<2:25:56, 11.96s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 3.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_194_aug_3.jpg:  32% 335/1035 [50:48<1:21:04,  6.95s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.44 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 4.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_194.jpg:  33% 342/1035 [51:55<1:54:25,  9.91s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.44 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 4.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_468_aug_3.jpg:  38% 395/1035 [59:22<1:22:30,  7.74s/it]      inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_246_aug_3.jpg:  39% 407/1035 [1:01:20<1:15:17,  7.19s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 6.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_246_aug_0.jpg:  40% 416/1035 [1:02:37<1:36:09,  9.32s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 6.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561.jpg:  41% 428/1035 [1:04:07<1:11:28,  7.06s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 3.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353_aug_6.jpg:  42% 437/1035 [1:05:30<1:20:48,  8.11s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_356.jpg:  44% 459/1035 [1:08:38<1:14:45,  7.79s/it]      inference failed, error: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_818_aug_7.jpg:  45% 461/1035 [1:09:01<1:27:44,  9.17s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_818_aug_1.jpg:  48% 499/1035 [1:14:45<1:12:39,  8.13s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_194_aug_0.jpg:  49% 508/1035 [1:16:09<1:01:17,  6.98s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.44 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 4.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_356_aug_6.jpg:  49% 511/1035 [1:16:34<1:08:11,  7.81s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 3.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_246_aug_6.jpg:  50% 520/1035 [1:17:46<56:12,  6.55s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 6.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353.jpg:  51% 533/1035 [1:19:39<1:19:55,  9.55s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_545_aug_5.jpg:  55% 567/1035 [1:24:33<1:03:38,  8.16s/it]      inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 6.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353_aug_5.jpg:  56% 578/1035 [1:26:10<50:11,  6.59s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_468_aug_0.jpg:  57% 585/1035 [1:27:06<52:16,  6.97s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_468_aug_5.jpg:  58% 603/1035 [1:29:40<38:22,  5.33s/it]      inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_901_aug_7.jpg:  59% 611/1035 [1:30:54<44:54,  6.36s/it]      inference failed, error: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 6.09 GiB is allocated by PyTorch, and 5.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_194_aug_6.jpg:  60% 616/1035 [1:31:37<47:50,  6.85s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.44 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 4.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_818_aug_3.jpg:  60% 619/1035 [1:32:05<51:57,  7.49s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 3.68 GiB is free. Process 1570959 has 12.09 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353_aug_4.jpg:  72% 742/1035 [1:49:46<49:11, 10.07s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 5.06 GiB is free. Process 1570959 has 10.71 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 3.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_818_aug_2.jpg:  76% 783/1035 [1:55:42<34:06,  8.12s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 5.06 GiB is free. Process 1570959 has 10.71 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 2.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_356_aug_4.jpg:  76% 786/1035 [1:56:12<34:29,  8.31s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 4.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_829_aug_6.jpg:  78% 810/1035 [1:59:31<24:09,  6.44s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 6.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_356_aug_0.jpg:  79% 814/1035 [2:00:12<33:20,  9.05s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 4.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_545_aug_3.jpg:  82% 846/1035 [2:05:10<25:01,  7.94s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 7.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_545_aug_2.jpg:  82% 847/1035 [2:05:20<27:18,  8.72s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 7.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_194_aug_4.jpg:  84% 867/1035 [2:07:44<18:57,  6.77s/it]      inference failed, error: CUDA out of memory. Tried to allocate 5.44 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 6.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_901_aug_1.jpg:  84% 870/1035 [2:08:11<20:22,  7.41s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 6.09 GiB is allocated by PyTorch, and 7.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_545_aug_0.jpg:  84% 871/1035 [2:08:22<23:14,  8.50s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 7.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_356_aug_5.jpg:  86% 889/1035 [2:10:51<18:26,  7.58s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 4.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_818_aug_5.jpg:  86% 892/1035 [2:11:29<24:46, 10.40s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 5.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_829_aug_4.jpg:  87% 896/1035 [2:12:15<25:26, 10.98s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 6.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_3.jpg:  87% 899/1035 [2:12:48<24:13, 10.69s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 5.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_246_aug_2.jpg:  90% 930/1035 [2:17:10<17:32, 10.03s/it]inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 7.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353_aug_1.jpg:  91% 944/1035 [2:19:32<13:39,  9.01s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 6.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_459.jpg:  92% 952/1035 [2:20:31<08:43,  6.30s/it]     inference failed, error: CUDA out of memory. Tried to allocate 4.94 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 6.50 GiB is allocated by PyTorch, and 6.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_1.jpg:  94% 968/1035 [2:22:52<08:29,  7.60s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 5.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_0.jpg:  95% 987/1035 [2:25:34<05:45,  7.20s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 5.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_468_aug_6.jpg:  97% 1007/1035 [2:28:35<03:16,  7.02s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 5.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_545.jpg:  98% 1013/1035 [2:29:16<01:53,  5.17s/it]           inference failed, error: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 7.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_561_aug_7.jpg:  98% 1014/1035 [2:29:25<02:14,  6.40s/it]inference failed, error: CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 7.95 GiB is allocated by PyTorch, and 5.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_marchingband_1_353_aug_0.jpg:  99% 1024/1035 [2:30:58<01:51, 10.16s/it]inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 6.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_829.jpg: 100% 1031/1035 [2:32:00<00:38,  9.65s/it]      inference failed, error: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacty of 15.77 GiB of which 2.25 GiB is free. Process 1570959 has 13.52 GiB memory in use. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 6.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "../aug_widerface/AUG_WIDER_val/images/0--Parade/0_Parade_Parade_0_12_aug_7.jpg: 100% 1035/1035 [2:32:30<00:00,  8.84s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSs8y1Ba6Jny",
        "outputId": "542afee4-225c-467c-fa77-6ca76c9bcbaa"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r output.zip -X output -q"
      ],
      "metadata": {
        "id": "UINk01P1NzJj"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv output.zip drive/MyDrive/colab_mount/aug_widerface_infer.zip"
      ],
      "metadata": {
        "id": "VwATOT2kVIHm"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RoAsFz77BHFp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}